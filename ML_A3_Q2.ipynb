{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_A3_Q2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ6waZsZPv6V"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1SzyI5ACB79je-pBPlQm4Yy-thoOTP2bt?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zhs67Bn9PgdT"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import KFold\n",
        "import autograd.numpy as npy\n",
        "from autograd import grad\n",
        "from autograd import elementwise_grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6j1GvblLQSYU"
      },
      "source": [
        "class LogisticRegression():\n",
        "    def __init__(self, fit_intercept=True):\n",
        "        '''\n",
        "        :param fit_intercept: Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).\n",
        "        '''\n",
        "        self.fit_intercept = fit_intercept\n",
        "        self.coef_ = None #Replace with numpy array or pandas series of coefficients learned using using the fit methods\n",
        "        self.thetas = [] ##weights\n",
        "        self.costs = [] ##cost of iterations\n",
        "        self.regu = None\n",
        "        self.lamda = 0.1\n",
        "        self.X = None\n",
        "        self.y = None\n",
        "\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y, n_iter=100000, lr=0.01, lr_type='constant',regu = None,lamda = 0.1):\n",
        "        \n",
        "        if isinstance(X,pd.DataFrame):\n",
        "            X = X.to_numpy()\n",
        "        \n",
        "        if isinstance(y,pd.Series):\n",
        "            y = y.to_numpy()\n",
        "\n",
        "        m = X.shape[1]\n",
        "        n = len(y)  \n",
        "        y = y.reshape(n,1)\n",
        "        if self.fit_intercept == True:\n",
        "            theta = np.random.randn(m+1,1)\n",
        "            X0 = np.ones((n,1))\n",
        "            X = np.append(X0,X,axis = 1)\n",
        "        elif self.fit_intercept == False:\n",
        "            theta = np.random.randn(m,1)\n",
        "\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "        ## For \n",
        "        # n = no. of samples\n",
        "        # m = no. of features\n",
        "        # X : n x m\n",
        "        # theta : m x 1\n",
        "        # y : n x 1\n",
        "\n",
        "        for it in range(1,n_iter+1):\n",
        "            if lr_type == 'constant':\n",
        "                c = 1\n",
        "            elif lr_type == 'inverse':\n",
        "                c = 1/(it+1)\n",
        "\n",
        "            Z = np.dot(X,theta)  \n",
        "            y_hat = self.sigmoid(Z)\n",
        "            # cost = self.cost(y,y_hat)\n",
        "            dtheta = (1/n)*np.dot( X.T,y_hat-y) \n",
        "            theta = theta - c*lr*dtheta\n",
        "\n",
        "            # Keeping track of our weights\n",
        "            self.thetas.append(theta)\n",
        "            \n",
        "            # Keeping track of our cost function value\n",
        "            # self.costs.append(cost)\n",
        "\n",
        "        self.coef_ = theta\n",
        "        return\n",
        "\n",
        "    def sigmoid(self,Z):\n",
        "        return 1/(1+npy.exp(-Z))\n",
        "\n",
        "    def cost(self,theta): \n",
        "        X = self.X\n",
        "        y = self.y\n",
        "        Z = npy.dot(X,theta)  \n",
        "        y_hat = self.sigmoid(Z)\n",
        "\n",
        "        y1 = y*npy.log(y_hat)\n",
        "        y2 = (1-y)*npy.log(1-y_hat)\n",
        "        # cost = -(1/len(y))*npy.sum( y*npy.log(y_hat) + (1-y)*npy.log(1-y_hat))\n",
        "        cost = -(1/len(y))*npy.sum(y1 + y2)\n",
        "        if(self.regu==\"L1\"):\n",
        "            cost += self.lamda*(npy.sum(npy.absolute(theta)))\n",
        "        elif(self.regu==\"L2\"):\n",
        "            cost += self.lamda*(npy.sum(npy.square(theta)))\n",
        "        else:\n",
        "            return cost\n",
        "\n",
        "        return cost\n",
        "\n",
        "    def fit_autograd(self, X, y, n_iter=10000, lr=0.01, lr_type='constant',regu = None,lamda = 0.1):\n",
        "\n",
        "        if isinstance(X,pd.DataFrame):\n",
        "            X = X.to_numpy()\n",
        "        \n",
        "        if isinstance(y,pd.Series):\n",
        "            y = y.to_numpy()\n",
        "\n",
        "        m = X.shape[1]\n",
        "        n = len(y)  \n",
        "        y = y.reshape(n,1)\n",
        "        if self.fit_intercept == True:\n",
        "            theta = np.random.randn(m+1,1)\n",
        "            X0 = np.ones((n,1))\n",
        "            X = np.append(X0,X,axis = 1)\n",
        "        elif self.fit_intercept == False:\n",
        "            theta = np.random.randn(m,1)\n",
        "\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "        ag = elementwise_grad(self.cost)\n",
        "        for it in range(1,n_iter+1):\n",
        "            if lr_type == 'constant':\n",
        "                c = 1\n",
        "            elif lr_type == 'inverse':\n",
        "                c = 1/(it+1)\n",
        "\n",
        "            cost = ag(theta)\n",
        "            \n",
        "            theta -= c*lr*cost\n",
        "\n",
        "            # Keeping track of our weights\n",
        "            self.thetas.append(theta)\n",
        "            \n",
        "            # Keeping track of our cost function value\n",
        "            self.costs.append(cost)\n",
        "            \n",
        "        self.coef_ = theta\n",
        "        return\n",
        "\n",
        "    \n",
        "    def predict(self, X):\n",
        "            \n",
        "        if isinstance(X,pd.DataFrame):\n",
        "            X = X.to_numpy()\n",
        "\n",
        "        if self.fit_intercept == True:  \n",
        "            m = X.shape[0]\n",
        "            X0 = np.ones((m,1))\n",
        "            X = np.append(X0,X,axis = 1)\n",
        "        \n",
        "        y_hat = self.sigmoid(X.dot(self.coef_))\n",
        "        y_hat = y_hat.reshape(len(y_hat),)\n",
        "        # print(y_hat)\n",
        "        for i in range(len(y_hat)):\n",
        "                if(y_hat[i]<=0.5):\n",
        "                    y_hat[i]=0\n",
        "                else:\n",
        "                    y_hat[i]=1\n",
        "        return pd.Series(y_hat)\n",
        "\n",
        "    def accuracy(self,y_hat,y):\n",
        "\n",
        "        assert(y_hat.size == y.size)\n",
        "        true_pred = 0\n",
        "        for i in range(y_hat.size):\n",
        "            if y_hat[i]==y[i]:\n",
        "                true_pred+=1\n",
        "\n",
        "        accu = true_pred/y.size\n",
        "        return accu*100\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yA5yge-RmR6"
      },
      "source": [
        "### Dataset Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CdpBt44Rrog"
      },
      "source": [
        "cancer = load_breast_cancer()\n",
        "\n",
        "X = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
        "y = pd.Series(cancer['target'])\n",
        "features = cancer.feature_names\n",
        "## Normalization of X\n",
        "for column in X.columns:\n",
        "    X.loc[:,column] = (X.loc[:,column] - X.loc[:,column].min())/(X.loc[:,column].max() - X.loc[:,column].min())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thdVCedize0L"
      },
      "source": [
        "### Part - a"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3H9AYZDzgs0",
        "outputId": "15296601-544a-41e9-bfb1-55f483425a55"
      },
      "source": [
        "regus = [\"L1\",\"L2\"]\n",
        "\n",
        "for regu in regus:\n",
        "    print(\"\\nUsing Regularisation\",regu)\n",
        "    clf = LogisticRegression()\n",
        "    clf.fit(X,y,regu=regu,lamda=0.1)\n",
        "    pred = clf.predict(X)\n",
        "    print(\"Accuracy: \",clf.accuracy(pred,y))\n",
        "\n",
        "    clf = LogisticRegression()\n",
        "    clf.fit_autograd(X,y,regu=regu,lamda=0.1)\n",
        "    pred = clf.predict(X)\n",
        "    print(\"Accuracy(Autograd): \",clf.accuracy(pred,y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Using Regularisation L1\n",
            "Accuracy:  97.18804920913884\n",
            "Accuracy(Autograd):  94.37609841827768\n",
            "\n",
            "Using Regularisation L2\n",
            "Accuracy:  97.89103690685414\n",
            "Accuracy(Autograd):  94.02460456942003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olrMZKAL3Y-s"
      },
      "source": [
        "### Part - b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i0fqLxJ3aW0"
      },
      "source": [
        "def q_b(regu,lamdas,X,y):\n",
        "  print(\"For Regularisation\",regu,\":\")\n",
        "  cnt = 1\n",
        "# L1_range = [i for i in np.arange(0.01,0.1,0.01)]\n",
        "  kf = KFold(n_splits=3,shuffle=True)\n",
        "  for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    kf1 = KFold(n_splits=3,shuffle=True)\n",
        "    accu = {}\n",
        "    for i in lamdas:\n",
        "        accu[i] = 0\n",
        "\n",
        "    for train_ind, test_ind in kf1.split(X_train):\n",
        "        X_tn, X_tt = X[train_ind], X[test_ind]\n",
        "        y_tn, y_tt = y[train_ind], y[test_ind]\n",
        "        for lamda in lamdas:\n",
        "            LR = LogisticRegression()\n",
        "            LR.fit_autograd(X_tn,y_tn,regu=regu,lamda=lamda)\n",
        "            yh = LR.predict(X_tt)\n",
        "            accu[lamda] += LR.accuracy(yh, y_tt)\n",
        "\n",
        "    vals = list(accu.values())\n",
        "    best_accu = max(vals)\n",
        "    best_lamda = lamdas[vals.index(max(vals))]\n",
        "\n",
        "    LR = LogisticRegression()\n",
        "    LR.fit_autograd(X_train,y_train,regu=regu,lamda=best_lamda)\n",
        "\n",
        "    y_hat = LR.predict(X_test)\n",
        "    ac = LR.accuracy(y_hat,y_test)\n",
        "    print(cnt,\". Value of optimum penalty term lambda\",best_lamda,\"giving accuracy\",ac)\n",
        "\n",
        "    if regu == \"L1\":\n",
        "        theta = list(np.array(LR.coef_).reshape(-1))\n",
        "        l = [(abs(theta[i]),i) for i in range(1,len(theta))]\n",
        "        l = sorted(l, reverse = True)\n",
        "        print(\"\\t Following are the Three Most Important features :\")\n",
        "        print(\"\\t\\t >\",features[l[0][1]-1])\n",
        "        print(\"\\t\\t >\",features[l[1][1]-1])\n",
        "        print(\"\\t\\t >\",features[l[2][1]-1])\n",
        "\n",
        "    cnt +=1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVDwmjtg3yV0",
        "outputId": "752075a9-b387-47ce-8094-9e28922eb07e"
      },
      "source": [
        "q_b(\"L1\",[i for i in np.arange(0.01,0.1,0.02)],X.to_numpy(),y.to_numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For Regularisation L1 :\n",
            "1 . Value of optimum penalty term lambda 0.01 giving accuracy 92.63157894736842\n",
            "\t Following are the Three Most Important features :\n",
            "\t\t > mean concavity\n",
            "\t\t > concavity error\n",
            "\t\t > worst concave points\n",
            "2 . Value of optimum penalty term lambda 0.03 giving accuracy 93.6842105263158\n",
            "\t Following are the Three Most Important features :\n",
            "\t\t > mean area\n",
            "\t\t > worst perimeter\n",
            "\t\t > texture error\n",
            "3 . Value of optimum penalty term lambda 0.06999999999999999 giving accuracy 93.65079365079364\n",
            "\t Following are the Three Most Important features :\n",
            "\t\t > worst perimeter\n",
            "\t\t > mean concave points\n",
            "\t\t > worst concave points\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wxlp0Rhi_CRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48d8aa28-2cd2-409b-b02e-68b6032e80c7"
      },
      "source": [
        "q_b(\"L2\",[i for i in np.arange(0.01,0.1,0.02)],X.to_numpy(),y.to_numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For Regularisation L2 :\n",
            "1 . Value of optimum penalty term lambda 0.01 giving accuracy 90.52631578947368\n",
            "2 . Value of optimum penalty term lambda 0.08999999999999998 giving accuracy 94.21052631578948\n",
            "3 . Value of optimum penalty term lambda 0.08999999999999998 giving accuracy 94.17989417989418\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}